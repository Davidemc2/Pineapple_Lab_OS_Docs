# ğŸ”§ Modular Prompt Engineering Blueprint for Custom AI Agents

## ğŸ§  Core Philosophy

**LLMs â‰  Thinkers.** They simulate logic, language, and reasoning. The goal of prompt engineering is to design interfaces that enable reliable, domain-specific utility â€” with humans providing ambiguity resolution and high-context judgement.

---

## ğŸ”— Purpose of This Guide

This guide is part of an evolving set of findings from Pineapple Labs. I'm David Edwards, and this blueprint represents my ongoing exploration into prompt engineering as it relates to the broader field of AI engineering.

The idea behind publishing this is to open-source the design patterns, techniques, and structures I've found useful when working with LLMs like GPT, Claude, and DeepSeek. Prompting isnâ€™t just about formatting text â€” itâ€™s about shaping behavior, logic, and reliability inside intelligent systems.

The goal is to make these findings public, collaborative, and useful â€” not just for Pineapple Labs, but for builders, researchers, and technologists everywhere who care about designing more trustworthy, performant agents.

---

## ğŸ§± 1. SYSTEM DECLARATION (Agent Role Definition)

```text
You are a [role] with expertise in [domain]. Be [tone]. Avoid assumptions. Ask for clarification if needed.
```

**Tips:**

* Be explicit: "You are a certified tax advisor..."
* Reinforce desired behavior: "Be concise and regulatory-compliant."

---

## ğŸ“¥ 2. INSTRUCTION BLOCK

```text
Your task is to [perform task] using the input provided below.
```

**Clarify Scope:**

* â€œ...without rewriting the original textâ€
* â€œ...within 200 wordsâ€

---

## ğŸ” 3. LOGIC BLOCK (Step-by-Step or Conditional Reasoning)

```text
Step 1: [action]
Step 2: [action]
If [condition], then [output], else [alternative]
```

**Patterns:**

* Chain-of-thought style prompts
* Decision trees (e.g., refund logic)

---

## ğŸ§© 4. INPUT/OUTPUT SEPARATION

```text
## Instructions:
[clear task]

## Input Data:
[data/doc/user query]

## Expected Output Format:
[bullets / JSON / table / narrative]
```

**Prevents:** blending task logic and input context

---

## ğŸ¯ 5. OUTPUT FORMATTING & CONSTRAINTS

```text
Respond in [JSON/table/bullets]. Limit to [X] words. Include [required fields].
```

**Example:**

```text
Return a JSON: { "summary": string, "score": 0-10, "warning": boolean }
```

---

## ğŸ§ª 6. FEW-SHOT EXAMPLES

```text
Input: "yo help me"
Output: "Hi, could you please clarify what you need help with?"
```

**Use cases:**

* Style learning
* Format anchoring
* Tone control

---

## ğŸš¨ 7. EVALUATION & ESCAPE HATCHES

### ğŸ” Failure Modes

* Ambiguous input â†’ vague output
* Over-specification â†’ rigid, non-generalizable output
* Lack of output constraints â†’ verbosity or hallucinations
* Conflict between system prompt and instruction â†’ breakdown in task logic

### âœ… Escape Hatches

```text
If you are uncertain, or cannot complete the task with the input provided, say:
"I need clarification to proceed: [insert specific clarification request]"
```

**Why:** This enables graceful degradation and safe fail states. Especially important for GPT-4 and Claude.

### ğŸ“Š Evaluation Methods

Use this checklist to test prompt reliability:

* âœ… 3x consistency on regenerate
* âœ… Valid JSON or Markdown formatting
* âœ… Passes tone/style alignment checks
* âœ… Logical flow mirrors intended reasoning chain
* âœ… No hallucinations in factual queries
* âœ… Latency and token cost within expected bounds

---

## ğŸ”¢ VERSIONING PHILOSOPHY

Prompts evolve like software.

```json
{
  "prompt_id": "legal_summary_v1.2.0",
  "changes": "Added tone constraint + updated examples",
  "owner": "pineapple-labs"
}
```

**Best Practices:**

* `MAJOR.MINOR.PATCH` structure (v2.1.3)
* Keep changelogs per prompt
* Store prompt history (e.g., Chroma, GitHub, Pinecone metadata)

---

## ğŸ§  Model-Specific Prompting Notes

### ğŸ¤– GPT-4

* âœ… Best for multi-role simulation, creative tasks, code
* ğŸ”§ Use structured prompts with logic + format constraints
* ğŸ§  Supports embedded feedback loops

### ğŸ§  Claude 3/4

* âœ… Excels in structured legal, policy-safe tasks
* ğŸ”§ Use `<tag>` style XML formatting for consistent behavior
* âš ï¸ Avoid indirect instructions â€” prefers strict format

### ğŸ” DeepSeek

* âœ… High performance in technical/code domains
* ğŸ”§ Prompt like GPT, but test output determinism
* âš ï¸ Needs constraint tuning to avoid overgeneration

---

## ğŸ›  Reusable Prompt Snippets (Prompt Blocks)

| Block Type       | Snippet                                               |
| ---------------- | ----------------------------------------------------- |
| System Context   | "You are an expert \[role]. Be precise. Never guess." |
| Task Instruction | "Your task is to \[action] using the info below."     |
| Formatting       | "Return JSON: {question, answer, confidence}"         |
| Clarification    | "If you are uncertain, ask for more information."     |
| Logic Flow       | "Step 1... Step 2... Step 3..."                       |

---

## ğŸ§¬ Real-World Prompt Blueprints

### ğŸ› Legal Compliance GPT

```text
You are a compliance advisor for financial regulation. Summarize key infractions and cite the relevant law.
Input: [legal document]
Output: JSON: { violations: [], citations: [], summary }
```

### ğŸ“ Support Chat Optimizer

```text
You are a tier-2 support agent. Rephrase the issue into a format for engineering triage.
Input: [customer message]
Output: JSON: { issue_summary, urgency_level, component_tag }
```

### ğŸ“Š Analyst Assistant GPT

```text
You are a data analyst at a SaaS company. Identify three insights based on growth metrics.
Input: [monthly performance report]
Output: Markdown: - Insight 1: ...
```

---

## ğŸ” Prompt Tuning Lifecycle

1. ğŸ— Write â†’
2. ğŸ”¬ Evaluate â†’
3. ğŸ” Regenerate â†’
4. ğŸ§ª Test for Consistency â†’
5. ğŸ“Š Deploy to Agent Memory â†’
6. ğŸ§­ Auto-Tune via Reflection (e.g., ReAct/AutoGen loops)

---

## ğŸ“˜ Example Blueprint

```text
## SYSTEM
You are a climate policy analyst.
Be evidence-based and non-partisan.

## INSTRUCTION
Summarize the document below and extract 3 policy impacts.

## LOGIC
Step 1: Read and extract summary.
Step 2: Identify direct impacts.
Step 3: Output formatted list.

## INPUT
[Insert document text here]

## FORMAT
Return JSON with keys: summary, impacts[], confidence_score

## EVALUATION CLAUSE
If source is unclear, state "Document lacks clarity on policy outcome."
```

---

## ğŸ”— Sources & References

* [Anthropic Claude Prompting Best Practices](https://docs.anthropic.com/claude/prompt-engineering)
* [OpenAI Function Calling & Prompting Docs](https://platform.openai.com/docs)
* [Deepgram Prompt Engineering Masterclass](https://deepgram.com/learn/prompt-engineering-masterclass)
* [Prompt Engineering: A Blueprint for AI Excellence](https://www.crossml.com/wp-content/uploads/2024/02/Prompt-Engineering-A-Blueprint-for-AI-Excellence.pdf)
* *Designing Machine Learning Systems* by Chip Huyen
* *Deep Learning* by Ian Goodfellow et al.
* *Prompt Engineering Guide* ([https://github.com/dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide))
* Pineapple Labs Internal Research â€“ OS Brick runtime & behavior design

